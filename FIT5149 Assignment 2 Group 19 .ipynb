{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sentiment Classification for Product Reviews\n",
    "\n",
    "\n",
    "### Date: 2nd June, 2019\n",
       ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing required Libraries for Pre-Processing, Feature Extraction and Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.util import ngrams\n",
    "from itertools import chain\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from  sklearn import  metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading Train, Labels and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trn_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trn_1</td>\n",
       "      <td>Well this place got me to write my first revie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trn_2</td>\n",
       "      <td>A very good Greek restaurant with tasty food. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trn_3</td>\n",
       "      <td>Website says open, Google says open, Yelp says...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trn_4</td>\n",
       "      <td>If I could give zero stars I would. When we wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trn_5</td>\n",
       "      <td>They have great food &amp; definitely excellent se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trn_id                                               text\n",
       "0  trn_1  Well this place got me to write my first revie...\n",
       "1  trn_2  A very good Greek restaurant with tasty food. ...\n",
       "2  trn_3  Website says open, Google says open, Yelp says...\n",
       "3  trn_4  If I could give zero stars I would. When we wa...\n",
       "4  trn_5  They have great food & definitely excellent se..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train Dataset\n",
    "train=pd.read_csv(\"train_data.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trn_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trn_1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trn_2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trn_3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trn_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trn_5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trn_id  label\n",
       "0  trn_1      2\n",
       "1  trn_2      5\n",
       "2  trn_3      1\n",
       "3  trn_4      1\n",
       "4  trn_5      5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Dataset\n",
    "train_labels=pd.read_csv(\"train_label.csv\")\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Dataset\n",
    "test=pd.read_csv(\"test_data.csv\")\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing\n",
    "Steps followed for data preprocessing:\n",
    "* Tokenization\n",
    "* Extracting Bigram Collocations\n",
    "* Performing Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_tokens=[] # Empty list to store all the extracted tokens\n",
    "\n",
    "def Tokenizedata(ids,raw_text): # Function to tokenize the datasets\n",
    "    text = raw_text.lower()  # Converting text into lowercase\n",
    "    text = re.sub(r'\\d+','',raw_text) # Removing digits \n",
    "    emojis = re.findall(r'(:\\)|:\\(|:\\-\\(|:\\-\\)|:\\*|:\\-\\*|:\\/|:\\-\\/|;\\)|:D)',text) # Extracting emojis\n",
    "    y = RegexpTokenizer(r\"[\\w']+\") # Tokenizing words\n",
    "    tokens = y.tokenize(text)\n",
    "    Final_tokens = emojis + tokens # Appending all the lists of tokens into final tokens list\n",
    "    return (ids,Final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.1 PREPROCESSING FOR TRAIN DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling Tokenizedata function to perform tokenization on train dataset and converting it into dictionary\n",
    "tokenized_reuters =  dict(Tokenizedata(row['trn_id'],row['text']) for idx,row in train.iterrows()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tokens into list\n",
    "words = list(chain.from_iterable(tokenized_reuters.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Bigram Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding top 800 Bigram Collocations by calculating Pointwise Mutual Information (PMI)\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures() \n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "top_words=finder.nbest(bigram_measures.pmi, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list for Parts of Speech (POS) Tags\n",
    "POS_TAGS=['JJ','JJR','JJS','RB','RBR','RBS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tagged=[] # Empty list to store tags\n",
    "for i in top_words:\n",
    "    tagged_sent = nltk.tag.pos_tag([i[0].strip('\\''),i[1].strip('\\'')]) # Performing POS tagging on extracted bigram collocations\n",
    "    if tagged_sent[0][1] in POS_TAGS or tagged_sent[1][1] in POS_TAGS:\n",
    "        Tagged.append((tagged_sent[0][0],tagged_sent[1][0])) # Appending tagged bigrams containing adjectives and adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using Multi-Word Expression Tokenizer to include in the dictioanry\n",
    "mwetokenizer = MWETokenizer(Tagged) # including bi-grams in the dictionary\n",
    "colloc_units =  dict((key, mwetokenizer.tokenize(values)) for key,values in tokenized_reuters.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into dataframe\n",
    "df = pd.DataFrame(colloc_units.items(), columns=['Trn_id', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Trn_id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trn_1</td>\n",
       "      <td>['Well', 'this', 'place', 'got', 'me', 'to', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trn_2</td>\n",
       "      <td>[':)', 'A', 'very', 'good', 'Greek', 'restaura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trn_3</td>\n",
       "      <td>['Website', 'says', 'open', 'Google', 'says', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trn_4</td>\n",
       "      <td>['If', 'I', 'could', 'give', 'zero', 'stars', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trn_5</td>\n",
       "      <td>['They', 'have', 'great', 'food', 'definitely'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Trn_id                                               Text\n",
       "0  trn_1  ['Well', 'this', 'place', 'got', 'me', 'to', '...\n",
       "1  trn_2  [':)', 'A', 'very', 'good', 'Greek', 'restaura...\n",
       "2  trn_3  ['Website', 'says', 'open', 'Google', 'says', ...\n",
       "3  trn_4  ['If', 'I', 'could', 'give', 'zero', 'stars', ...\n",
       "4  trn_5  ['They', 'have', 'great', 'food', 'definitely'..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting lists to strings\n",
    "df['Text']=df['Text'].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PREPROCESSING FOR TEST DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling Tokenizedata function to perform tokenization on test dataset and converting it into dictionary\n",
    "tokenized_reuters_test =  dict(Tokenizedata(row['test_id'],row['text']) for idx,row in test.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tokens into list\n",
    "words_test = list(chain.from_iterable(tokenized_reuters_test.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Bigram Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding top 800 Bigram Collocations by calculating Pointwise Mutual Information (PMI)\n",
    "bigram_measures_test = nltk.collocations.BigramAssocMeasures()\n",
    "finder_test = nltk.collocations.BigramCollocationFinder.from_words(words_test)\n",
    "top_words_test=finder_test.nbest(bigram_measures_test.pmi, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tagged_test=[] # Empty list to store tags\n",
    "for i in top_words_test: \n",
    "    tagged_sent = nltk.tag.pos_tag([i[0].strip('\\''),i[1].strip('\\'')]) # Performing POS tagging on extracted bigram collocations\n",
    "    if tagged_sent[0][1] in POS_TAGS or tagged_sent[1][1] in POS_TAGS:\n",
    "        Tagged_test.append((tagged_sent[0][0],tagged_sent[1][0])) # Appending tagged bigrams containing adjectives and adverbs\n",
    "#Tagged_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Multi-Word Expression Tokenizer to include in the dictioanry\n",
    "mwetokenizer_test = MWETokenizer(Tagged_test) # including bi-grams in the dictionary\n",
    "colloc_units_test =  dict((key, mwetokenizer_test.tokenize(values)) for key,values in tokenized_reuters_test.items()) \n",
    "#colloc_units_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into dataframe\n",
    "df_test=pd.DataFrame(colloc_units_test.items(), columns=['Test_id', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_1</td>\n",
       "      <td>['trying', 'to', 'have', 'a', 'nice', 'quiet',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_2</td>\n",
       "      <td>['Been', 'getting', 'food', 'to', 'go', 'from'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_3</td>\n",
       "      <td>['Ugh', \"I've\", 'had', 'to', 'eat', 'here', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_4</td>\n",
       "      <td>[':)', 'The', 'people', 'here', 'are', 'so', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_5</td>\n",
       "      <td>['Heard', 'alot', 'of', 'good', 'things', 'abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Test_id                                               Text\n",
       "0  test_1  ['trying', 'to', 'have', 'a', 'nice', 'quiet',...\n",
       "1  test_2  ['Been', 'getting', 'food', 'to', 'go', 'from'...\n",
       "2  test_3  ['Ugh', \"I've\", 'had', 'to', 'eat', 'here', 'a...\n",
       "3  test_4  [':)', 'The', 'people', 'here', 'are', 'so', '...\n",
       "4  test_5  ['Heard', 'alot', 'of', 'good', 'things', 'abo..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting lists to strings\n",
    "df_test['Text']=df_test['Text'].astype(str)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating modified list of stop words to be removed\n",
    "remove_words=['and','i','to','a','it','that','yet','they','we','but','this','had','has','were','me','myself','yourself','saw','see','my','an','there','he','she','him','then','get','got','you','us','your','we']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN AND TEST DATA SPLIT ON TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(df['Text'], train_labels['label'], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three types of feature extractions have been performed:\n",
    "- In first feature set, bigrams are added.\n",
    "- In second feature set, trigrams are added.\n",
    "- In third feature set, basic pre-processing is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Feature Set1 - Adding Bi-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model using Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vectorizer', CountVectorizer(remove_words,ngram_range=(1,2),token_pattern=r'\\w{3,}',strip_accents='unicode')),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "        input=['and', 'i', 'to', 'a', 'it', 'that', 'yet', 'they', 'we', 'but', 'this', 'had', 'has', 'were', 'me', 'myself', 'yourself', 'saw', 'see', '..._class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=None))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train_p, y_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.76      0.75     39097\n",
      "           2       0.54      0.53      0.54     38517\n",
      "           3       0.53      0.50      0.52     38897\n",
      "           4       0.55      0.55      0.55     39406\n",
      "           5       0.72      0.74      0.73     39083\n",
      "\n",
      "   micro avg       0.62      0.62      0.62    195000\n",
      "   macro avg       0.62      0.62      0.62    195000\n",
      "weighted avg       0.62      0.62      0.62    195000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test_p, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6195333333333334"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_p,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vectorizer', CountVectorizer(remove_words,ngram_range=(1,2),token_pattern=r'\\w{3,}',strip_accents='unicode')),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=True)),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(random_state=0)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chai0005\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "        input=['and', 'i', 'to', 'a', 'it', 'that', 'yet', 'they', 'we', 'but', 'this', 'had', 'has', 'were', 'me', 'myself', 'yourself', 'saw', 'see', '...state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train_p, y_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting\n",
    "pred = model.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.79      0.76     39097\n",
      "           2       0.58      0.55      0.56     38517\n",
      "           3       0.57      0.53      0.55     38897\n",
      "           4       0.58      0.57      0.57     39406\n",
      "           5       0.73      0.77      0.75     39083\n",
      "\n",
      "   micro avg       0.64      0.64      0.64    195000\n",
      "   macro avg       0.64      0.64      0.64    195000\n",
      "weighted avg       0.64      0.64      0.64    195000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test_p, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6424615384615384"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_p,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: MultinomiaL Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vectorizer', CountVectorizer(remove_words,ngram_range=(1,2),token_pattern=r'\\w{3,}',strip_accents='unicode')),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=True)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "        input=['and', 'i', 'to', 'a', 'it', 'that', 'yet', 'they', 'we', 'but', 'this', 'had', 'has', 'were', 'me', 'myself', 'yourself', 'saw', 'see', '...ifier(estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "          n_jobs=None))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train_p, y_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting \n",
    "pred = model.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.73      0.67     39097\n",
      "           2       0.49      0.53      0.51     38517\n",
      "           3       0.50      0.55      0.53     38897\n",
      "           4       0.53      0.55      0.54     39406\n",
      "           5       0.83      0.51      0.63     39083\n",
      "\n",
      "   micro avg       0.57      0.57      0.57    195000\n",
      "   macro avg       0.59      0.57      0.57    195000\n",
      "weighted avg       0.59      0.57      0.57    195000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test_p, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5728564102564102"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test_p,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature Set 2 - Adding Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tri-grams on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()   ## Trigrams\n",
    "finder_tri = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
    "top_words_tri=finder_tri.nbest(trigram_measures.pmi, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('White', 'mexican'),\n",
       " ('faux', 'boeuf'),\n",
       " ('heavy', 'greasy'),\n",
       " ('high', 'class'),\n",
       " ('other', 'country'),\n",
       " ('sign', 'here'),\n",
       " ('sweet', 'thyme'),\n",
       " ('Anniversary', 'celebrating'),\n",
       " ('Catholic', 'Guilt'),\n",
       " ('Corporate', 'Shopper'),\n",
       " ('Family', 'Circus'),\n",
       " ('German', 'Section'),\n",
       " ('Heavy', 'Indian'),\n",
       " ('Hispanic', 'twist'),\n",
       " ('INCOMPETENT', 'STAFF'),\n",
       " ('Incident', 'Report'),\n",
       " ('Invisible', 'Excavating'),\n",
       " ('Lady', 'Madonna'),\n",
       " ('Local', 'Asian'),\n",
       " ('Lower', 'Manhattan'),\n",
       " ('Noble', 'Intentions'),\n",
       " ('OPEN', 'LATE'),\n",
       " ('Ordinary', 'People'),\n",
       " ('Original', 'Sin'),\n",
       " ('Postage', 'Due'),\n",
       " ('Slow', 'Movement'),\n",
       " ('Spanish', 'Fly'),\n",
       " ('Stiff', 'Slap'),\n",
       " ('Still', 'Standing'),\n",
       " ('Top', 'Drag'),\n",
       " ('Traditional', 'Poutine'),\n",
       " ('Tropical', 'Smoothie'),\n",
       " ('Urban', 'Grocery'),\n",
       " ('Vegetarian', 'Vegetarian'),\n",
       " (\"Who's\", 'Responsible'),\n",
       " ('Whole', 'Paycheck'),\n",
       " ('Wild', 'Raspberry'),\n",
       " ('american', 'staple'),\n",
       " ('anniversary', 'weekend'),\n",
       " ('anytime', 'quinoa'),\n",
       " ('artistic', 'skills'),\n",
       " ('automatic', 'gratuity'),\n",
       " ('aztec', 'affection'),\n",
       " ('bish', 'slaps'),\n",
       " ('clothing', 'optional'),\n",
       " ('culinary', 'venture'),\n",
       " ('defective', 'radiator'),\n",
       " ('downscale', 'upscale'),\n",
       " ('due', 'diligence'),\n",
       " ('easy', 'peasy'),\n",
       " ('empty', 'calories'),\n",
       " ('en', 'mass'),\n",
       " ('essential', 'oils'),\n",
       " ('est', 'sacrilÃ'),\n",
       " ('exactly', 'matched'),\n",
       " ('fabulous', 'fajitas'),\n",
       " ('fallen', 'apart'),\n",
       " ('fattest', 'hairiest'),\n",
       " ('faux', 'Japanese'),\n",
       " ('finnish', 'fir'),\n",
       " ('frugal', 'budget'),\n",
       " ('funeral', 'agent'),\n",
       " ('ghetto', 'fabulous'),\n",
       " ('golden', 'sand'),\n",
       " ('greatest', 'hits'),\n",
       " ('huge', 'knockers'),\n",
       " ('illegal', 'alien'),\n",
       " ('incident', 'report'),\n",
       " ('interior', 'decorator'),\n",
       " ('irish', 'pennant'),\n",
       " ('kept', 'quiet'),\n",
       " ('legal', 'tender'),\n",
       " ('lightly', 'sweet'),\n",
       " ('lite', 'jalapeno'),\n",
       " ('locally', 'sourced'),\n",
       " ('mad', 'scientist'),\n",
       " ('official', 'names'),\n",
       " ('parallel', 'universe'),\n",
       " ('rocky', 'start'),\n",
       " ('romantic', 'couply'),\n",
       " ('roundish', 'square'),\n",
       " ('sloppy', 'joe'),\n",
       " ('smart', 'move'),\n",
       " ('sour', 'grapes'),\n",
       " ('sous', 'vide'),\n",
       " ('standard', 'candle'),\n",
       " ('steady', 'Mandy'),\n",
       " ('steal', 'billions'),\n",
       " ('sustainable', 'farming'),\n",
       " ('tallest', 'midget'),\n",
       " ('tasty', 'bits'),\n",
       " ('technical', 'difficulties'),\n",
       " ('tiny', \"tom's\"),\n",
       " ('true', 'pubs'),\n",
       " ('uncooked', 'texture'),\n",
       " ('undeliverable', 'destroyed'),\n",
       " ('underground', 'leaks'),\n",
       " ('universal', 'mandate'),\n",
       " ('upper', 'deck'),\n",
       " ('upset', 'stomach'),\n",
       " ('vegetable', 'biryani'),\n",
       " ('wide', 'release'),\n",
       " ('works', 'remotely'),\n",
       " ('Algerian', 'rai'),\n",
       " ('Amerindian', 'Numismatic'),\n",
       " ('Anspruch', 'genommen'),\n",
       " ('Antal', 'bukas'),\n",
       " ('Autonomic', 'Seizure'),\n",
       " ('BRestaurants', 'utm_term'),\n",
       " ('Biggly', 'biggly'),\n",
       " ('CONTINuous', 'ROUNDS'),\n",
       " ('Cantorial', 'Soloist'),\n",
       " ('Chicobean', 'BDaht'),\n",
       " ('Clar', 'Lounbar'),\n",
       " ('Congressional', 'Liasion'),\n",
       " ('faux', 'boeuf', 'MMHHHHH'),\n",
       " ('Algerian', 'rai', 'gnawa'),\n",
       " ('BRestaurants', 'utm_term', 'Bdivine'),\n",
       " ('Clar', 'Lounbar', 'Barounge'),\n",
       " ('Coimhead', 'fearg', 'fhear'),\n",
       " ('Gruppen', 'schonmal', 'jemanden'),\n",
       " ('Hipbilly', 'Hillyster', 'Hiphilly'),\n",
       " ('Ongelooflijk', 'lekkere', 'keuken'),\n",
       " ('THEchic', 'THEelegance', 'THEsassy'),\n",
       " ('__ngt__', 'TTdeaddaceaeaO_cZ', 'TGKSZcNPiQcx'),\n",
       " ('agv', 'alpinestars', 'dainese'),\n",
       " ('alatan', 'kelngan', 'sbayan'),\n",
       " ('anschliessend', 'vollends', 'glÃ¼cklich'),\n",
       " ('assanign', 'longliveAlexanderMcQueen', 'geturlife'),\n",
       " ('barnfamiljer', 'Hotellets', 'utsida'),\n",
       " ('burgerbingeweek', 'yelpgtaBBW', 'Yelpburgerbinge'),\n",
       " ('burgerkingdoesntdoglutenfree', 'esmereldawasajerk', 'illnevercomeback'),\n",
       " ('businessSearchTerm', 'SpeakGeeks', 'searchType'),\n",
       " ('chauhan', 'originindia', 'yolandaph'),\n",
       " ('cucchiaio', 'doveva', 'essere'),\n",
       " ('dachten', 'tatsÃ', 'chlich'),\n",
       " ('dalawang', 'empleyadong', 'nabaggit'),\n",
       " ('edelman', 'franco', 'sarto'),\n",
       " ('emballage', 'biodÃ', 'gradable'),\n",
       " ('esmereldawasajerk', 'illnevercomeback', 'burgerkingsucks'),\n",
       " ('figuresandsinglenokids', 'disposableincome', 'shoppingwhilebrown'),\n",
       " ('friendlystaff', 'familyoriented', 'greatdrinks'),\n",
       " ('gemieteten', 'Fahrzeug', 'einsteigen'),\n",
       " ('geturlife', 'tsapowertrip', 'thatshadeofgreenmakesyoulookugly'),\n",
       " ('heaveninmymouth', 'getinmybelly', 'thateggtho'),\n",
       " ('hoffentlich', 'Besseres', 'berichten'),\n",
       " ('hojos', 'howard', \"johnsons's\"),\n",
       " ('inbegrepen', 'Canadees', 'chique'),\n",
       " ('innergÃ', 'rden', 'Liveband'),\n",
       " ('isso', 'estrelas', 'senÃ'),\n",
       " ('kababesh', 'aliii', 'bood'),\n",
       " ('kartoffelsalat', 'rouladen', 'wiersingeintopf'),\n",
       " ('laajan', 'valikoiman', 'erilaisia'),\n",
       " ('lkiruokaherkkuja', 'Ainekset', 'ovat'),\n",
       " ('llebas', 'esas', 'Estupidas'),\n",
       " ('longliveAlexanderMcQueen', 'geturlife', 'tsapowertrip'),\n",
       " ('nVtSUauVErTCyAHykoGICA', 'ved', 'CAoQ_AUoAQ'),\n",
       " ('najbolji', 'Ukusna', 'hrana'),\n",
       " ('naprÃ', 'klad', 'prekvapila'),\n",
       " ('natsukashii', 'omoi', 'Arigatoo'),\n",
       " ('nderÃ¼bergreifen', 'auftritt', 'ZurÃ¼ck'),\n",
       " ('needsome', 'deepclean', 'whatif'),\n",
       " ('nei', 'vari', 'negozi'),\n",
       " ('nekolik', 'velikosti', 'pracek'),\n",
       " ('nglichkeiten', 'Abnutzungen', 'auftreten'),\n",
       " ('nickelanddimeyou', 'badboss', 'badbusiness'),\n",
       " ('nickjr', 'disneyjr', 'pbskids'),\n",
       " ('nopayless', 'Foxalltheway', 'Paymoreforgoodservice'),\n",
       " ('notthatPaulGiamattimovie', 'greatreviewsthough', 'Ishouldgiveitavew'),\n",
       " ('oiczippvkjdimu',\n",
       "  'isbrokendowhybdcxnTetkoiczippjdimudo',\n",
       "  'odoglilczippjdimkuxxy'),\n",
       " ('omalta', 'tilalta', 'tuotuja'),\n",
       " ('oute', 'erroR', 'wwr'),\n",
       " ('peppermilllasvegasnotcustomeroriented', 'nickelanddimeyou', 'badboss'),\n",
       " ('rskild', 'smiskstÃ', 'llning'),\n",
       " ('schonmal', 'jemanden', 'vorschicken'),\n",
       " ('sslicher', 'Kasten', 'grau'),\n",
       " ('tuoreita', 'suoraan', 'omalta'),\n",
       " ('ufiger', 'Defekte', 'Pannen'),\n",
       " ('umstÃ', 'ndlicher', 'RegelmÃ'),\n",
       " ('usar', 'desglosar', 'copiar'),\n",
       " ('utm_term', 'Bdivine', 'Beatery'),\n",
       " ('vegaslocal', 'ilovevegas', 'vegaslife'),\n",
       " ('vepro', 'knedlo', 'zelo'),\n",
       " ('verdere', 'spannende', 'dingen'),\n",
       " ('vorbereiteten', 'PortionsschÃ', 'lchen'),\n",
       " ('vstorewtfe', 'needsome', 'deepclean'),\n",
       " ('yezzy', 'ato', \"matsumo's\"),\n",
       " ('½ä¼¼woodbine', 'aveç', 'costcoå'),\n",
       " ('Katastrophe', 'unfreundlich', 'unkonzentriert'),\n",
       " ('Poweful', 'vacuumes', 'Picks'),\n",
       " ('Swissair', 'Inevitable', 'Unavoidable'),\n",
       " ('altre', 'ripiene', 'Questa'),\n",
       " ('artig', 'ihren', 'Dienst'),\n",
       " ('azfoodie', 'foodiegram', 'phxfoodie'),\n",
       " ('barbarous', 'uncivil', 'constuction'),\n",
       " ('betrachten', 'konnten', 'Wurden'),\n",
       " ('dahin', 'fÃ¼hren', 'soll'),\n",
       " ('deepclean', 'whatif', 'andthen'),\n",
       " ('endlich', 'durchgefuehrten', 'Renovierung'),\n",
       " ('entscheiden', 'Milchmann', 'Bauarbeiter'),\n",
       " ('erfassten', 'Daten', 'ausfÃ¼llen'),\n",
       " ('finns', 'uppstÃ', 'lld'),\n",
       " ('geral', 'fecha', 'cedo'),\n",
       " ('guga', 'huck', \"fedi's\"),\n",
       " ('hadden', 'verschillende', 'mensen'),\n",
       " ('livreurs', 'NOUS', 'SOMMES'),\n",
       " ('naprosto', 'spokojeny', 'Maj'),\n",
       " ('navigatedFrom', 'biznameSubmit', 'BName'),\n",
       " ('navigatedFrom', 'businessResultsDrillDown', 'mc_perm_id'),\n",
       " ('negativen', 'bisherigen', 'Berichten'),\n",
       " ('ngay', 'ddal', 'twens'),\n",
       " ('nteerde', 'eetzaakjes', 'Zelfs'),\n",
       " ('omizu', 'okudosai', 'Whooooo'),\n",
       " ('organicad', 'ifthx', 'kneadersgilbert'),\n",
       " ('ovAcABgIPBFBUiBYQKA', 'Gx', 'zZuvYynZbUDBlpA'),\n",
       " ('perpÃ', 'tuellement', 'renouvelÃ'),\n",
       " ('silentsuspekts', 'ssmedia', 'yyz'),\n",
       " ('specialitÃ', 'sembravano', 'avere'),\n",
       " ('standen', 'Helme', 'KopfhÃ'),\n",
       " ('taten', 'artig', 'ihren'),\n",
       " ('tengan', 'habre', 'vengan'),\n",
       " ('trotz', 'WÃ¼stengegend', 'schlechtes'),\n",
       " ('unfreundlich', 'unkonzentriert', 'falsche'),\n",
       " ('unzÃ', 'hligen', 'Touris'),\n",
       " ('unÂ', 'derÂ', 'whelm'),\n",
       " ('volete', 'soltanto', 'dormire'),\n",
       " ('wÃ¼nschen', 'ergÃ', 'nzen'),\n",
       " ('zentral', 'WohlfÃ¼hlfaktor', 'garantiert'),\n",
       " ('Ã¼berflÃ¼ssige', 'Geschirr', 'abzurÃ'),\n",
       " ('AbstÃ', 'nden', 'verkehrenden'),\n",
       " ('ELECTROTEL', 'JEAN', 'TALON'),\n",
       " ('JusT', 'ThE', 'WaY')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tagged_tri_train=[]    ## Trigrams\n",
    "for i in top_words_tri: \n",
    "    tagged_sent = nltk.tag.pos_tag([i[0].strip('\\''),i[1].strip('\\''),i[2].strip('\\'')])\n",
    "    if tagged_sent[0][1] in POS_TAGS or tagged_sent[1][1] in POS_TAGS or tagged_sent[2][1] in POS_TAGS:\n",
    "        Tagged.append((tagged_sent[0][0],tagged_sent[1][0],tagged_sent[2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer = MWETokenizer(Tagged) # including bi-grams and tri-grams in the dictionary\n",
    "colloc_units =  dict((key, mwetokenizer.tokenize(values)) for key,values in colloc_units.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(colloc_units.items(), columns=['train_id', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trn_1</td>\n",
       "      <td>['Well', 'this', 'place', 'got', 'me', 'to', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trn_2</td>\n",
       "      <td>[':)', 'A', 'very', 'good', 'Greek', 'restaura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trn_3</td>\n",
       "      <td>['Website', 'says', 'open', 'Google', 'says', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trn_4</td>\n",
       "      <td>['If', 'I', 'could', 'give', 'zero', 'stars', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trn_5</td>\n",
       "      <td>['They', 'have', 'great', 'food', 'definitely'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  train_id                                               Text\n",
       "0    trn_1  ['Well', 'this', 'place', 'got', 'me', 'to', '...\n",
       "1    trn_2  [':)', 'A', 'very', 'good', 'Greek', 'restaura...\n",
       "2    trn_3  ['Website', 'says', 'open', 'Google', 'says', ...\n",
       "3    trn_4  ['If', 'I', 'could', 'give', 'zero', 'stars', ...\n",
       "4    trn_5  ['They', 'have', 'great', 'food', 'definitely'..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting lists to strings\n",
    "df_train['Text']=df_train['Text'].astype(str)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tri-grams on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures_test = nltk.collocations.TrigramAssocMeasures()\n",
    "finder_tri_test = nltk.collocations.TrigramCollocationFinder.from_words(words_test)\n",
    "top_words_test_tri=finder_tri_test.nbest(trigram_measures_test.pmi, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('French', 'Vietnamese'),\n",
       " ('Little', 'Phillies'),\n",
       " ('Mexican', 'Seafood'),\n",
       " ('Summer', 'School'),\n",
       " (\"That's\", 'Italian'),\n",
       " ('_just_', 'water'),\n",
       " ('already', 'prepared'),\n",
       " ('blank', 'canvass'),\n",
       " ('crazy', 'fries'),\n",
       " ('deep', 'dish'),\n",
       " ('dive', 'bar'),\n",
       " ('extra', 'veggies'),\n",
       " ('flat', 'white'),\n",
       " ('gimme', 'more'),\n",
       " ('greasy', 'spoon'),\n",
       " ('happy', 'hour'),\n",
       " ('higher', 'end'),\n",
       " ('large', 'dog'),\n",
       " ('last', 'straw'),\n",
       " ('looked', 'forward'),\n",
       " ('lovely', 'cocktail'),\n",
       " ('mongolian', 'chicken'),\n",
       " ('more', 'protein'),\n",
       " ('never', 'mind'),\n",
       " ('new', 'hotel'),\n",
       " ('non', 'existent'),\n",
       " ('nueva', 'cocina'),\n",
       " ('nutty', 'classic'),\n",
       " ('other', 'job'),\n",
       " ('perfect', 'combination'),\n",
       " ('popular', 'location'),\n",
       " ('probably', 'sold'),\n",
       " ('reliable', 'carrier'),\n",
       " ('scale', 'test'),\n",
       " ('shattered', 'glass'),\n",
       " ('soft', 'critique'),\n",
       " ('thin', 'crusts'),\n",
       " ('tong', 'shui'),\n",
       " ('top', 'tier'),\n",
       " ('total', 'gym'),\n",
       " ('Advanced', 'Nutrients'),\n",
       " ('Anal', 'Seepage'),\n",
       " ('Architectural', 'Digest'),\n",
       " ('Awakened', 'Confident'),\n",
       " ('CONSISTENT', 'RESULTS'),\n",
       " ('CORPORATE', 'HEADQUATERS'),\n",
       " ('Calgarian', 'southie'),\n",
       " ('DROP', 'Elegantly'),\n",
       " ('DineSafeMain', 'userRequest'),\n",
       " ('Disposable', 'Income'),\n",
       " ('Doubly', 'endorsed'),\n",
       " ('Ethiopean', 'Eritrean'),\n",
       " ('Fukuda', 'funnily'),\n",
       " ('Gi', 'normous'),\n",
       " ('Glamorous', 'divas'),\n",
       " ('Grisons', 'Reallly'),\n",
       " ('Guilefully', 'dimidiated'),\n",
       " ('Honourable', 'Mentions'),\n",
       " ('Inglorious', 'Basterds'),\n",
       " ('Larchmere', 'Porchfest'),\n",
       " ('Lethal', 'Enforcers'),\n",
       " ('Lymphatic', 'Drainage'),\n",
       " ('Morbidly', 'insensitively'),\n",
       " ('Mortal', 'Kombat'),\n",
       " ('Musical', 'Inszenierung'),\n",
       " ('Namen', 'Tally'),\n",
       " ('Oral', 'Maxillofacial'),\n",
       " ('Parental', 'Advisory'),\n",
       " ('Petty', 'tributary'),\n",
       " ('Racial', 'slurs'),\n",
       " ('DineSafeMain', 'userRequest', 'view_history'),\n",
       " ('Lymphatic', 'Drainage', 'Cellulite'),\n",
       " ('ZWHEAqpFACPPA', 'utm_campaign', 'www_review_share_popup'),\n",
       " ('amenazan', 'acciones', 'legales'),\n",
       " ('butant', 'intermÃ', 'diaire'),\n",
       " ('dober', 'tek', 'Slovenian'),\n",
       " ('eneklerden', 'birisi', 'Tavuk'),\n",
       " ('estomac', \"d'acier\", 'Longue'),\n",
       " ('kaldere', 'klima', 'Var'),\n",
       " ('knabbern', 'Preislich', 'bewegt'),\n",
       " ('llega', 'gente', 'esperando'),\n",
       " ('lvbuffet', 'offthestrip', 'thestriplv'),\n",
       " ('nachlesen', 'konnte', 'handelt'),\n",
       " ('naznaÄ', 'iÅ', 'kto'),\n",
       " ('nent', 'pied', 'nu'),\n",
       " ('nnte', 'mir', 'nie'),\n",
       " ('nosili', 'jedlo', 'boli'),\n",
       " ('oRS', 'NWon', 'DlSiZqXxQ'),\n",
       " ('oWh', 'VqFsDb', 'Fcsw'),\n",
       " ('optisch', 'gÃ¼nstiger', 'erscheinen'),\n",
       " ('patella', 'femoral', 'malignment'),\n",
       " ('promÃ', 'nent', 'pied'),\n",
       " ('rambo', 'cambodian', 'refugee'),\n",
       " ('revoltingly', 'globulus', 'protuberance'),\n",
       " ('scrivere', 'questa', 'recensione'),\n",
       " ('serviert', 'DarÃ¼ber', 'hinaus'),\n",
       " ('soient', 'clairement', 'indiquÃ'),\n",
       " ('thelime', 'tamarindo', 'guayaba'),\n",
       " ('uk', 'xfj', 'ZWHEAqpFACPPA'),\n",
       " ('umbenannt', 'Kurze', 'Zeit'),\n",
       " ('unverschÃ', 'mt', 'hoch'),\n",
       " ('userRequest', 'view_history', 'ESTABLISHMENT_ID'),\n",
       " ('utm_campaign', 'www_review_share_popup', 'utm_medium'),\n",
       " ('verlÃ', 'uft', 'relativ'),\n",
       " ('www_review_share_popup', 'utm_medium', 'copy_link'),\n",
       " ('wÃ', 'hrend', 'unseres'),\n",
       " ('xfj', 'ZWHEAqpFACPPA', 'utm_campaign'),\n",
       " ('Å¾ete', 'doniesÅ', 'vlastnÃ½'),\n",
       " ('è¾¹ã', 'Fitting', 'roomæ'),\n",
       " ('DEFECTIVE', 'FURNITURE', 'DELIVERED'),\n",
       " ('GetrÃ', 'nkepulver', 'gemixt'),\n",
       " ('LIARS', 'CHEATERS', 'SCAMMERERS'),\n",
       " ('Myofacial', 'Release', 'Lymphatic'),\n",
       " ('aj', 'preto', 'Å¾e'),\n",
       " ('anstrengenden', 'Spaziergang', 'erholt'),\n",
       " ('danecegoodman', 'Desktop', 'IMG_'),\n",
       " ('echt', 'gute', 'QualitÃ'),\n",
       " ('ften', 'Unterhaltung', 'wird'),\n",
       " ('gute', 'Musical', 'Inszenierung'),\n",
       " ('homophobic', 'islamaphobic', 'isis'),\n",
       " ('lezzetli', 'Patates', 'beklenilenden'),\n",
       " ('mythical', 'wok', 'hei'),\n",
       " ('nuruengjitang', 'tangsuyuk', 'kampungki'),\n",
       " ('tten', 'mehr', 'erwartet'),\n",
       " ('udang', 'kacang', 'Pendek'),\n",
       " ('Mexican', 'Seafood', 'yields'),\n",
       " ('alle', 'Amis', 'lachen'),\n",
       " ('dÃ³lares', 'unas', 'clases'),\n",
       " ('inmediatamente', 'Hemos', 'tenido'),\n",
       " ('laut', 'sondern', 'tonal'),\n",
       " ('lich', 'erÃ', 'ffnete'),\n",
       " ('lua', 'Jambon', 'Pate'),\n",
       " ('nemala', 'veÄ¾kÃ½', 'zÃ'),\n",
       " ('otra', 'vez', 'Voy'),\n",
       " ('rencontrer', \"l'emmental\", 'fondu'),\n",
       " ('sensible', 'CABIN', 'FEVER'),\n",
       " ('seÃ', 'eneklerden', 'birisi'),\n",
       " ('simulated', 'meat', 'analogues'),\n",
       " ('tempÃ', 'rature', 'froide'),\n",
       " ('totalement', 'immangeable', 'Courageux'),\n",
       " ('free', 'show', 'hoodlums'),\n",
       " ('Automatic', 'Transmission', 'Shifting'),\n",
       " ('Awakened', 'Confident', 'Grateful'),\n",
       " ('Ich', 'kÃ', 'nnte'),\n",
       " ('Mutual', 'Bunk', 'Exhibition'),\n",
       " ('Optical', 'Lenses', 'Normality'),\n",
       " ('Oral', 'Maxillofacial', 'Surgery'),\n",
       " ('Untz', 'untz', 'untz'),\n",
       " ('aber', 'einmal', 'hin'),\n",
       " ('jian', 'guo', 'zhe'),\n",
       " ('liveauctioneers', 'invaluable', 'artnet'),\n",
       " ('llig', 'unnÃ', 'tig'),\n",
       " ('month', \"chef's\", 'special'),\n",
       " ('nomai', 'nem', 'kao'),\n",
       " ('nothingcanholdmeback', 'HEsgotmyback', 'Disclosure'),\n",
       " ('plural', 'vernacular', 'yinz'),\n",
       " ('schmeckte', 'wirklich', 'hervorragend'),\n",
       " ('spielen', 'wÃ¼rden', 'Und'),\n",
       " ('unserem', 'Bummel', 'durch'),\n",
       " ('unserem', 'morgendlichen', 'Spaziergang'),\n",
       " ('ven', 'buenas', 'paletas'),\n",
       " ('visuelles', 'abondent', 'Cependant'),\n",
       " ('wirklich', 'hervorragend', 'Allerdings'),\n",
       " ('Authentic', 'Neapolitan', 'Pizza'),\n",
       " ('duite', 'Cela', 'etant'),\n",
       " ('hier', 'unverschÃ', 'mt'),\n",
       " ('lindo', 'mechocan', 'macayos'),\n",
       " ('verre', 'afin', \"d'intensifier\"),\n",
       " ('wer', 'braucht', 'Fitnessroom'),\n",
       " ('werden', 'frisch', 'zubereitet'),\n",
       " ('ECO', 'evangelical', 'covenant'),\n",
       " ('Fully', 'Stocked', \"Ship's\"),\n",
       " ('Kosten', 'fÃ¼r', 'Tage'),\n",
       " ('Mmmmmmm', 'bowel', 'obstructingly'),\n",
       " ('Virtual', 'Reality', 'VR'),\n",
       " ('erÃ', 'ffnet', 'SchlieÃ'),\n",
       " ('fÃ¼r', 'meinen', 'Geschmack'),\n",
       " ('hatten', 'schon', 'besseres'),\n",
       " ('hervorragend', 'Allerdings', 'fÃ¼r'),\n",
       " ('laut', 'spielen', 'wÃ¼rden'),\n",
       " ('na', 'malÃ', 'papieriky'),\n",
       " ('na', 'tejto', 'reÅ'),\n",
       " ('nalievali', 'Obsluha', 'bola'),\n",
       " ('nihon', \"jin's\", 'repertoire'),\n",
       " ('noch', 'kÃ', 'lter'),\n",
       " ('ritable', 'breadfruit', 'Acra')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tagged_tri_test=[]\n",
    "for i in top_words_test_tri: \n",
    "    tagged_sent = nltk.tag.pos_tag([i[0].strip('\\''),i[1].strip('\\''),i[2].strip('\\'')])\n",
    "    if tagged_sent[0][1] in POS_TAGS or tagged_sent[1][1] in POS_TAGS or tagged_sent[2][1] in POS_TAGS:\n",
    "        Tagged_test.append((tagged_sent[0][0],tagged_sent[1][0],tagged_sent[2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwetokenizer_test = MWETokenizer(Tagged_test) # including bi-grams in the dictionary\n",
    "colloc_units_test =  dict((key, mwetokenizer_test.tokenize(values)) for key,values in colloc_units_test.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.DataFrame(colloc_units_test.items(), columns=['Test_id', 'Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test_id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_1</td>\n",
       "      <td>['trying', 'to', 'have', 'a', 'nice', 'quiet',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_2</td>\n",
       "      <td>['Been', 'getting', 'food', 'to', 'go', 'from'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_3</td>\n",
       "      <td>['Ugh', \"I've\", 'had', 'to', 'eat', 'here', 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_4</td>\n",
       "      <td>[':)', 'The', 'people', 'here', 'are', 'so', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_5</td>\n",
       "      <td>['Heard', 'alot', 'of', 'good', 'things', 'abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Test_id                                               Text\n",
       "0  test_1  ['trying', 'to', 'have', 'a', 'nice', 'quiet',...\n",
       "1  test_2  ['Been', 'getting', 'food', 'to', 'go', 'from'...\n",
       "2  test_3  ['Ugh', \"I've\", 'had', 'to', 'eat', 'here', 'a...\n",
       "3  test_4  [':)', 'The', 'people', 'here', 'are', 'so', '...\n",
       "4  test_5  ['Heard', 'alot', 'of', 'good', 'things', 'abo..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting lists to strings\n",
    "df_test['Text']=df_test['Text'].astype(str)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(df_train['Text'], train_labels['label'], test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1:SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vectorizer', CountVectorizer(remove_words,ngram_range=(1,2),token_pattern=r'\\w{3,}',strip_accents='unicode')),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=True,norm=\"l1\")),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train_p, y_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting\n",
    "pred = model.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test_p, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2:Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vectorizer', CountVectorizer(remove_words,ngram_range=(1,2),token_pattern=r'\\w{3,}',strip_accents='unicode')),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=True,norm=\"l1\")),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(random_state=0)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train_p, y_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting\n",
    "pred = model.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test_p, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('vectorizer', CountVectorizer(remove_words,ngram_range=(1,2),token_pattern=r'\\w{3,}',strip_accents='unicode')),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True,sublinear_tf=True,norm=\"l1\")),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train_p, y_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting\n",
    "pred = model.predict(X_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test_p, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Feature Set 3 - Basic Pre-processing\n",
    "\n",
    "### STEPS PERFORMED \n",
    "- Basic <b> data preprocessing</b> has been performed on the provided train dataset.\n",
    "- Using <b>cross validation </b>, train dataset has been divided into train and test datasets.\n",
    "- Various <b> models </b> have been created and <b> accuracy </b> has been checked using predicted values on test dataset created using CV.\n",
    "- Finally, <b> labels are predcited </b> for the provided test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Train, Labels and Test Datasets again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "train=pd.read_csv('train_data.csv',encoding=\"UTF-8\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "test=pd.read_csv('test_data.csv',encoding=\"UTF-8\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Dataset\n",
    "train_labels=pd.read_csv(\"train_label.csv\",encoding=\"UTF-8\")\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'] = train_labels['label']\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train[['label','text']]\n",
    "test_df = test[['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def preprocess(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\n\", \"\", string)    \n",
    "    string = re.sub(r\"\\r\", \"\", string) \n",
    "    string = re.sub(r\"[0-9]\", \"digit\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in train_df.iterrows():\n",
    "    data = str(row['text'])\n",
    "    new_data = preprocess(data)\n",
    "    train_df.set_value(i,'text',new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in test_df.iterrows():\n",
    "    data = str(row['text'])\n",
    "    new_data = preprocess(data)\n",
    "    test_df.set_value(i,'text',new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "X = []\n",
    "\n",
    "from  sklearn.cross_validation import train_test_split\n",
    "\n",
    "for i in train_df['text']:\n",
    "    X.append(i)\n",
    "\n",
    "y = np.array(train_df[\"label\"])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models and Checking the accuracy using split Datasets\n",
    "- For creating models, pipelining is used. It is a method to streamline a code while adding more features. \n",
    "- It simplifies the process of manually running through each step like vectorizing, tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for SVC Model using variou parameters\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting to the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Logistic Model using variou parameters\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(random_state=0)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Multinomial NB Model using variou parameters\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Labels for provided Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for SVC Model using variou parameters\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model\n",
    "model.fit(train_df['text'], train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_df['text'][0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred_svc = model.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting to dataframe\n",
    "predDf_svc = pd.DataFrame(pred_svc)\n",
    "predDf_svc['test_id'] = test['test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf_svc.columns = ['label','test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing into csv\n",
    "import csv\n",
    "predDf_svc.to_csv(r'pred_svc.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Logistic Model using variou parameters\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(random_state=0)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the mddel\n",
    "model.fit(train_df['text'], train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_df['text'][0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = model.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting to dataframe\n",
    "predDf = pd.DataFrame(pred)\n",
    "predDf['test_id'] = test['test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf.columns = ['label','test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing into csv\n",
    "import csv\n",
    "predDf.to_csv(r'predict_label.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for Multinomial NB Model using variou parameters\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the mddel\n",
    "model.fit(train_df['text'], train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = test_df['text'][0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred_multi = model.predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting to dataframe\n",
    "predDf_multi = pd.DataFrame(pred_multi)\n",
    "predDf_multi['test_id'] = test['test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDf_multi.columns = ['label','test_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing into csv\n",
    "import csv\n",
    "predDf_multi.to_csv(r'pred_multi.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
